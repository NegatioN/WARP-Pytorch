---

title: WARP-Pytorch

keywords: fastai
sidebar: home_sidebar

summary: "An implementation of WARP loss which uses matrixes and stays on the GPU in PyTorch."
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: index.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
    
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>An implementation of WARP loss which uses matrixes and stays on the GPU in PyTorch.</p>
<p>This means instead of using a for-loop to find the first offending negative sample that ranks above our positive,
we compute all of them at once. Only later do we find which sample is the first offender, and compute the loss with
respect to this sample.</p>
<p>The advantage is that it can use the speedups that comes with GPU-usage.</p>
<h2 id="When-is-WARP-loss-advantageous?">When is WARP loss advantageous?<a class="anchor-link" href="#When-is-WARP-loss-advantageous?">&#182;</a></h2><p>If you're ranking items or making models for recommendations, it's often advantageous to let your loss function directly
optimize for this case. WARP loss looks at 1 explicit positive up against the implicit negative items that a user never sampled,
and allows us to adjust weights of the network accordingly.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Install">Install<a class="anchor-link" href="#Install">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>pip install warp_loss</code></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="How-to-use">How to use<a class="anchor-link" href="#How-to-use">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The loss function requires scores for both positive examples, and negative examples to be supplied, such as in the example below.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="k">class</span> <span class="nc">OurModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_labels</span><span class="p">,</span> <span class="n">emb_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">OurModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">num_labels</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">user_embs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">neg</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">neg</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">one_user_vector</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">user_embs</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">())</span>
        <span class="n">repeated_user_vector</span> <span class="o">=</span> <span class="n">one_user_vector</span><span class="o">.</span><span class="n">repeat</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">pos_res</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">emb</span><span class="p">(</span><span class="n">pos</span><span class="p">),</span> <span class="n">repeated_user_vector</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">neg_res</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">emb</span><span class="p">(</span><span class="n">neg</span><span class="p">),</span> <span class="n">repeated_user_vector</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">pos_res</span><span class="p">,</span> <span class="n">neg_res</span>
        
<span class="n">num_labels</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">OurModel</span><span class="p">(</span><span class="n">num_labels</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pos_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">high</span><span class="o">=</span><span class="n">num_labels</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># our five labels</span>
<span class="n">neg_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">high</span><span class="o">=</span><span class="n">num_labels</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span> <span class="c1"># a few random negatives per positive</span>

<span class="n">pos_res</span><span class="p">,</span> <span class="n">neg_res</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">pos_labels</span><span class="p">,</span> <span class="n">neg_labels</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Positive Labels:&#39;</span><span class="p">,</span> <span class="n">pos_labels</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Negative Labels:&#39;</span><span class="p">,</span> <span class="n">neg_labels</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Model positive scores:&#39;</span><span class="p">,</span> <span class="n">pos_res</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Model negative scores:&#39;</span><span class="p">,</span> <span class="n">neg_res</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">warp_loss</span><span class="p">(</span><span class="n">pos_res</span><span class="p">,</span> <span class="n">neg_res</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="n">num_labels</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Loss:&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Positive Labels: tensor([[65],
        [94],
        [21]])
Negative Labels: tensor([[ 8, 45],
        [37, 93],
        [88, 84]])
Model positive scores: tensor([[-3.7806],
        [-1.9974],
        [-4.1741]], grad_fn=&lt;SqueezeBackward1&gt;)
Model negative scores: tensor([[-1.5696, -4.4905],
        [-1.9300, -0.3826],
        [ 2.4564, -2.1741]], grad_fn=&lt;SqueezeBackward1&gt;)
Loss: tensor(54.7226, grad_fn=&lt;SumBackward0&gt;)
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;We can also see that the gradient is only active for 2x the number of positive labels:&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">emb</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Meaning we correctly discard the gradients for all other than the offending negative label.&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>We can also see that the gradient is only active for 2x the number of positive labels: 6
Meaning we correctly discard the gradients for all other than the offending negative label.
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Assumptions">Assumptions<a class="anchor-link" href="#Assumptions">&#182;</a></h2><p>The loss function assumes you have already sampled your negatives randomly.</p>
<p>As an example this could be done in your dataloader:</p>
<ol>
<li>Assume we have a total dataset of 100 items</li>
<li>Select a positive sample with index 8</li>
<li>Your negatives should be a random selection from 0-100 excluding 8.</li>
</ol>
<p>Ex input to loss function: model scores for pos: [8] neg: [88, 3, 99, 7]</p>
<p>Currently only tested on PyTorch v0.4</p>
<h3 id="References">References<a class="anchor-link" href="#References">&#182;</a></h3><ul>
<li><a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37180.pdf">WSABIE: Scaling Up To Large Vocabulary Image Annotation</a></li>
<li><a href="https://medium.com/@gabrieltseng/intro-to-warp-loss-automatic-differentiation-and-pytorch-b6aa5083187a">Intro to WARP loss - Automatic differentiation and PyTorch</a></li>
<li><a href="https://github.com/lyst/lightfm">LightFM</a> as a reference implementaiton</li>
</ul>

</div>
</div>
</div>
</div>
 

